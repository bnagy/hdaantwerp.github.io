{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chi-squared test for independence: Exercise chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and inspecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = pd.read_csv(\"../../datasets/chat/chat.tsv\", sep=\"\\t\")\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new categorical variable, called \"submission\", indicating whether the participant's submission to the research project was \"large\" (>500 tokens of chat data) or \"small\" (<=500 tokens):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat[\"submission\"] = chat.nr_tokens.apply(lambda x: \"large\" if x > 500 else \"small\")\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-squared test for independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So much for what you already knew... Let us now proceed to a chi-squared test for independence. First, we formulate the hypotheses, i.e. a **null hypothesis $H_0$** and an **alternative hypothesis $H_1$**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ##### Question:\n",
    "> What would your $H_0$ and $H_1$ be, based on the submissiontable (gender vs submission size)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we perform a chi-squared test for independence, **we must test its assumptions**:\n",
    "- are the data independent? (i.e., no repeated measurements)\n",
    "- is the sample size sufficiently large or do we need a continuity correction?\n",
    "- are the expected cell counts sufficiently large?\n",
    "\n",
    "Let's start by testing the **first assumption: are there indeed no repeated measurements in the dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.subject_ID.is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, no repeated measurements! Each subject occurs exactly once in the dataset. In case we did encounter repeated measurements, we would not be allowed to perform a chi-squared test... Instead, we would have to use something else - we'll come back to that in the final session of this class.\n",
    "\n",
    "Now, let's test the **second assumption: is the sample size sufficiently large?** A rule of thumb is that we want a sample size that is larger than 60 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.shape[0] > 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, sample size is large enough! We will not need to 'tweak' our chi-squared test. If the sample size is too small, we would need a continuity correction (see below).\n",
    "\n",
    "Finally, let's test the **third assumption: are the expected cells sufficiently large?**  All *expected* cell counts should be larger than or equal to 5. Note the emphasis on *expected*: it is a widely spread misconception that the *observed* cell counts need to be larger than 5... (Of course, if the observed cells are very small, the expected cells are more likely to be small too).\n",
    "\n",
    "Once you have a result object from `sp.stats.chi2_contingency`, you can inspect the expected cell counts with `res.expected_freq`, or calculate them yourself, which we'll do below.\n",
    "\n",
    "Note that for smaller counts, you can use other tests instead of a chi-squared test. For example a Fisher's Exact Test (`sp.stats.fisher_exact()`). If you have fewer than 1000 samples, it is often recommended to use Fisher, since there are fewer assumptions that need to be checked. The downside is that Fisher is a conservative test, so it may not have the statistical *power* to find a significant effect where one exists, and that **for Python there is no well-tested version of Fisher for tables bigger than 2x2**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE There are two widely performed chi-square tests. The test for *independence* (what we are doing here) can usually be arranged as a \"contingency table\". For two variables with two categories, this is a 2x2 table. For this kind of test, it is *much easier* to use `scipy.stats.chi2_contingency()`.\n",
    "\n",
    "> A related test when comparing two frequency histograms is the chi-square test for goodness-of-fit, found as `scipy.stats.chisquare()`. Although it is possible to use that test to get the same result, there is a confusing process called \"adjusting the degrees of freedom\". We might talk about the goodness-of-fit test another time. For now, just remember which is the right API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an argument, we give the **table** derived from our dataset (using `pd.crosstab`). The argument `correction` is set to false, since the sample size is large enough. If `correction` is true, a so-called **continuity correction** is applied: Gries (pp. 170-171 and 184-185) recommends to do so only when the sample size is small (*n* <= 60). When the sample size is larger, no additional correction is required. Some statisticians (me) recommend *always* using Yates correction when it applies, which is **only for 2x2 contingency tables**. The worst thing that will happen is that your $p$-value will be a little too conservative, but for large samples the correction will make almost no difference. The default for this option in scipy is `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pd.crosstab(chat.gender, chat.submission)\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = sp.stats.chi2_contingency(obs, correction=False)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, looking at the `expected_freq` property of the result object, expected cell counts are large enough!\n",
    "\n",
    "Since all three assumptions are met, the results of the test are valid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Chi2 Result: Statistic: {res.statistic.round(4)}, Degrees of Freedom: {res.dof}, p-value:{res.pvalue.round(4)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ##### Question:\n",
    "> How to interpret this result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: The $p$-value for $\\chi^2$ tests is *very* dependent on the degrees of freedom parameter. Our $\\chi^2$ statistic (about 9) is very unlikely given one degree of freedom, but if you increased the degrees just a little it would not be significant. Below we have plotted the CDF for $\\chi^2$ for a few different degrees of freedom. Note how for 1 (the correct parameter here) almost all of the probability mass is to the left (so the result is unlikely), but the cumulative mass at the same x-value for 9 degrees is only around 0.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You won't be expected to reproduce anything like this, but it doesn't actually use many tricks you\n",
    "# haven't learned :)\n",
    "\n",
    "cols = list(\"123456789\")\n",
    "X = np.linspace(0, 10, 1000)\n",
    "yvals = [sp.stats.chi2.cdf(X, dof) for dof in range(1, 10)]\n",
    "wide_df = pd.DataFrame(dict(zip(cols, yvals)))\n",
    "wide_df[\"xvals\"] = X\n",
    "long_df = wide_df.melt([\"xvals\"], var_name=\"dof\")\n",
    "sns.lineplot(long_df, x=\"xvals\", y=\"value\", hue=\"dof\", linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures of effect size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the chi-square value and the p-value are dependent on sample size, it is very wise to provide a measure of effect size too.\n",
    "\n",
    "Start by calculating odds ratio. First, calculate the **odds** for female and male participants of submitting a large donation. You find all necessary input for calculating these odds in your crosstab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# odds of \"large submission\" when gender = male:\n",
    "# (male_large) / (male_small)\n",
    "odds_male = 241 / 426\n",
    "print(odds_male)\n",
    "\n",
    "# odds of \"large submission\" when gender = female\n",
    "# (female_large) / (female_small)\n",
    "odds_female = 316 / 401\n",
    "print(odds_female)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the **odds ratio**, i.e. odds_female / odds_male. Note that you can also calculate it the other way around (odds_male/odds_female). You either get a score between 0 and 1, or between 1 and +inf. Both scores are equivalent (they are each other's inverse). The closer both scores are to 1, the smaller the effect size, and the further away they are from 1, the larger the effect size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = odds_female / odds_male\n",
    "ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_ratio = odds_male / odds_female\n",
    "inv_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ##### Question:\n",
    "> How to interpret this ratio? Note that the version of the odds ratio expressed between 1 and +inf is usually the most intuitive one to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: In more complex analysis, and especially where the odds are extremely small, or the samples are very large, or both, it is common to use **log-odds**. Dealing with odds as logarithms has a number of excellent mathematical advantages: the results are more *numerically stable*, and they can be multiplied (as normal odds) by *addition as logarithms*. They are, unfortunately, (even) more difficult to understand intuitively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, let's calculate another measure of effect size: **Cramer's V**. Cramer's V is a normalized version of the chi-squared value (normalized for sample size).\n",
    "Here is the formula: \n",
    "\n",
    "> Cramer's V = $\\Large \\sqrt{\\frac{\\chi^2}{n * (min(nrow,ncols) -1)}}$\n",
    "\n",
    "In which *n* is the sample size. To obtain the sample size, add margins to the crosstab `margins=True`. To obtain the number of rows and columns in your data table, either visually inspect the table, or use `shape` property *of the crosstab* (**not** the entire dataset!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.shape[0]  # n samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.shape  # (nrow, ncols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### EXERCISE\n",
    "> Now just fill in all the variables and convert the formula above from 'maths' to 'code'..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, scipy already has an API for a number of common measures of association, including Cramer's V, under `sp.stats.contingency.association`. You should look up the documentation :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.stats.contingency.association(obs, method=\"cramer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ##### Question:\n",
    "> How to interpret this score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-squared test (bis): recalculating by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This part is optional**: for those of you interested in how Python calculates the chi-squared test and what's behind it (actually, it's all pretty intuitive!). Feel free to read through this part to get a better understanding of the test. However, **we do not expect you to perform these manual calculations at the exam**. What you *should* be able to do at the exam: inspect and visualize the data, perform a chi-squared test with `chi2_contingency()` (don't forget to test all three assumptions first!), correctly interpret the results, and finally provide the relevant measures of effect size.\n",
    "\n",
    "Let's now go to the manual calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you calculate the **expected frequencies** (based on $H_0$: no correlation between gender and submission size)? The formula to calculate the expected count for each cell is: \n",
    "\n",
    "> $\\Large \\frac{\\sum_i{row_i} \\times \\sum_i{col_i}}{n}$\n",
    "\n",
    "Get the row and column totals first with `margins=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(chat.gender, chat.submission, margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use these to calculate the expected frequencies. Compare your result to the expected frequencies calculated automatically by the `chi2_contingency()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_female_large = (717 * 557) / 1384\n",
    "expected_female_small = (717 * 827) / 1384\n",
    "expected_male_large = (667 * 557) / 1384\n",
    "expected_male_small = (667 * 827) / 1384\n",
    "\n",
    "E = np.array(\n",
    "    [\n",
    "        [expected_female_large, expected_female_small],\n",
    "        [expected_male_large, expected_male_small],\n",
    "    ]\n",
    ")\n",
    "print(E)\n",
    "print(sp.stats.chi2_contingency(obs, correction=False).expected_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you calculate the **chi-squared value** by hand? Calculate the sum of the differences between observed and expected count per cell in the table:\n",
    "\n",
    "> $\\Large \\chi^2 = \\sum_{i=1}^{n}{\\frac{(O_i - E_i)^2}{E_i}}$\n",
    "\n",
    "Compare to the chi-squared value obtained by `chi2_contingency` (use the `.statistic` property of the result object). You should get the same result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O = obs.to_numpy()\n",
    "\n",
    "sum = 0.0\n",
    "# Enumerate the four indices for a 2d matrix, [0][0], [0][1], etc\n",
    "for i in [0, 1]:\n",
    "    for j in [0, 1]:\n",
    "        # sum +=  <-- change the code here\n",
    "\n",
    "print(f\"By hand: {sum}, scipy: {res.statistic}, Equal: {sum==res.statistic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand calculating $p$-values and critical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TIP: CHECK THE DOCS! ALWAYS CHECK THE DOCS!\n",
    "\n",
    "Most of the continuous distributions in `sp.stats` have a very similar API, so learning one helps you to learn them all. In this case, the docs for the chi-square distribution are [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check whether the chi-squared value you calculated is sufficiently large for the differences between observed and expected frequencies to be significant. \n",
    "You can check the **critical value** (i.e. the value that our chi-squared value needs to exceed for the correlation to be significant) in a chi-squared table with `sp.stats.chi2.isf()`. This is the 'inverse survival function', and it maps percentiles ((1 - CDF), which is often called the *survival function*) to statistic values.\n",
    "\n",
    "Provide:\n",
    "- the p-value (0.05 for humanities)\n",
    "- degrees of freedom = **$(nrows-1) * (ncols-1)$**\n",
    "\n",
    "Then compare the values: your previously obtained chi-squared value should be larger than the critical value for the correlation to be significant at p <= 0.05. It might also help to look at the CDF plot above for the line `dof=1` for an x-value of just under 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.stats.chi2.isf(0.05, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chi-squared value is indeed larger than the critical value! So we know that expected and observed frequencies differ from each other significantly with p <= 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, obtain an **exact p-value** for the chi-squared test with `sp.stats.chi2.sf()`. This is the *survival function*, which we have seen before, and it maps statistics to $p$-values (more accurately, to 1 - CDF())\n",
    "\n",
    "Provide:\n",
    "- the obtained chi-squared value\n",
    "- the degrees of freedom\n",
    "\n",
    "Compare the obtained p-value to the p-value output by `chi2_contingency()`. They should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.stats.chi2.sf(res.statistic, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.pvalue == sp.stats.chi2.sf(res.statistic, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Version History\n",
    "\n",
    "Current: v1.0.2\n",
    "\n",
    "30/9/24: 1.0.0: first draft, BN\n",
    "7/10/24: v 1.0.1: clarify some points on Fisher's test and Yates' correction, BN\n",
    "08/10/24: v 1.0.2: typos + proofread, MK\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
