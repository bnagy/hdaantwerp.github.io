{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79589327",
   "metadata": {},
   "source": [
    "# Model selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab9af46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568bd1dc",
   "metadata": {},
   "source": [
    "In the previous sprint, we covered several aspects of `ols()` and illustrated them on the basis of the reaction times datatset offered by Gries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10f3047",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../datasets/gries/05-2_reactiontimes.csv\", sep=\"\\t\")\n",
    "df[\"LENGTH\"] = df.CASE.str.len()\n",
    "# Level our categoricals so statsmodels can see them. Unless we use them in something like difference\n",
    "# encoding it won't make much model difference, but it can make a lot of interpretability\n",
    "# difference.\n",
    "df[\"IMAGEABILITY\"] = df.IMAGEABILITY.astype(\n",
    "    pd.api.types.CategoricalDtype(categories=[\"lo\", \"hi\"], ordered=True)\n",
    ")\n",
    "df[\"FAMILIARITY\"] = df.FAMILIARITY.astype(\n",
    "    pd.api.types.CategoricalDtype(categories=[\"lo\", \"med\", \"hi\"], ordered=True)\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74f9cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = df.columns[2:]\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9163dc",
   "metadata": {},
   "source": [
    "We discussed how independent variables (whether they were numeric, binary or categorical) could be used and combined to predict the reaction time as our main dependent variable. We saw how we can also combine these predictors (using a `+` in the formula notation), since they all seem to have some worth in modelling RT. We could even combine them all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deffc9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols(f\"RT ~ {' + '.join(preds)}\", data=df).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305d56c2",
   "metadata": {},
   "source": [
    "Interestingly, as a whole, this model is well below the critical threshold for a statistical significance, but most of the estimates for the parameters now appear to be insignificant (apart from the simple LENGTH of a word). This is confusing and raises the issue: which predictors should we include?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da99bd7",
   "metadata": {},
   "source": [
    "## Parameter Selection\n",
    "\n",
    "If we have the opportunity to include multiple variables in a linear model, we'll need a way to determine whether it's a good idea to add them. In other words, we'll need strategies for **model selection**, i.e. determining the \"best\" model for our data. There are basically two options:\n",
    "\n",
    "1. Forward selection: you start with a small dataset and gradually add more predictors, if these prove \"useful\".\n",
    "2. Backward selection: you start out with a huge model that covers many predictors, and you gradually remove predictors that don't appear to be \"useful\".\n",
    "\n",
    "Several views exist on this matter. Many people will consider it bad practice to test for predictors that weren't theoretically motivated in the first place, but heeding that advice, it can become difficult to discover new things. Both strategies ultimately have the same goal: \"Arrive at a **minimally adequate model**\", i.e. the smallest model that gives you an optimal fit. As a rule of thumb, smaller model are *always* to be prefered (cf. Occam's Razor). How do you determine whether a predictor is \"useful\"? There are two \"metrics\", that can be used:\n",
    "\n",
    "1. Look at $p$-values (if they are significant you keep them in). The problem is that you still end up with many variables that are all somewhat significant, but in a very large model.\n",
    "2. Work with a **criterion based approach**, using for instance the **Akaike Information Criterion** to compare models with and without a predictor. The nice property of AIC is that it penalizes models on the basis of their size (the number of predictors and parameters): the AIC for a model with *many* parameters will be automatically adjusted downwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971342c5",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "SIDEBAR AIC and BIC\n",
    "\n",
    "You can read more then you ever wanted to about the AIC [on wikipedia](https://en.wikipedia.org/wiki/Akaike_information_criterion), but the main things we want you to remember are:\n",
    "- lower AIC (or BIC) are good\n",
    "- AIC is only useful to compare **models for the same thing**. It makes no sense as an absolute scale (ie don't compare the AIC for different datasets or different dependent variables)\n",
    "- AIC (and even more so BIC) already penalize model parameters, so *as a rule of thumb* if adding a parameter increases $R^2$ and decreases AIC it is probably worthwhile\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f42bb79",
   "metadata": {},
   "source": [
    "## Buckaroo Datascience\n",
    "\n",
    "**For the sake of argument** let's look at a kind of hacky way to compare models. Since our prediction formula can be specified as a string, we can modify the specification string, and refit the model with every combination of parameters. The code below is a little challenging, so just think about the intuition.\n",
    "\n",
    "We we take *all subsets* from our five predictors, (from one feature to four, and *all combinations at each size*), build a string from that set, and then record a few model criteria. Remember that both AIC and BIC are good ways to compare model performance, and a **lower score is better**. The (adjusted) $R^2$ tells us how much variability is explained, and a **higher score is better**. So, we'll just combine them to create two *ad hoc* measures of Model Goodness, which we call `R2AIC` and `R2BIC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f64ec156",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "\n",
    "res = []\n",
    "for subset in chain.from_iterable(combinations(preds, r) for r in range(1, len(preds))):\n",
    "    model = smf.ols(f\"RT ~ {' + '.join(subset)}\", data=df).fit()\n",
    "    res.append(\n",
    "        {\n",
    "            \"model\": \" + \".join(subset),\n",
    "            \"BIC\": model.bic,\n",
    "            \"AIC\": model.aic,\n",
    "            \"ADJR2\": model.rsquared_adj,\n",
    "            \"R2AIC\": 1 / model.aic * model.rsquared_adj,\n",
    "            \"R2BIC\": 1 / model.bic * model.rsquared_adj,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9def9b59",
   "metadata": {},
   "source": [
    "This gives us a `list` of `dict` objects, with identical keys. This is one of the (many) ways to build a pandas `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f3f7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416cfa7a",
   "metadata": {},
   "source": [
    "Now we can rank our models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510418b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(res).sort_values(\"R2AIC\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fb900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in pd.DataFrame(res).sort_values(\"R2AIC\", ascending=False).model.head(10):\n",
    "    print(f'\"{x}\",')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af20b51",
   "metadata": {},
   "source": [
    "... so the best model **by this ranking** has only three features. Thinking about some sanity checks:\n",
    "- There are many models with an AIC/BIC in the high 400s, so nothing exceptional is happening (many are much worse)\n",
    "- The Adjusted $R^2$ for the top model doesn't clearly stand out (it's not even the best)\n",
    "- This is the best combination, and also one of the most parsimonious \n",
    "\n",
    "So it seems fair. The second-ranked model is also worth a very close look -- we would probably need to go on and think hard about our predictors to make a choice. Some things to think about:\n",
    "- Is there a good theoretical *reason* to include `FAMILIARITY` -- it scored the best of the single predictor models?\n",
    "    - This can only be answered according to the scientific hypothesis motivating the study!\n",
    "- Is there 'overlap' between `FAMILIARITY` and `MEANINGFULNESS`? (the two-predictor model `FAMILIARITY + LENGTH` didn't do so well...)\n",
    "\n",
    "Some people would choose the leaner model on principle; some wouldn't. There is no right or wrong answer, it is a choice that you need to make using your scientific expertise, *and not just by looking at numbers*. Sorry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1d1287",
   "metadata": {},
   "source": [
    "Anyway, here are your two leading candidates. Choose your fighter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7a6e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols(\"RT ~ C(FAMILIARITY,Diff) + MEANINGFULNESS + LENGTH\", data=df).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb4b543",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols(\"RT ~ MEANINGFULNESS + LENGTH\", data=df).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158c81f3",
   "metadata": {},
   "source": [
    "```\n",
    "Version History\n",
    "\n",
    "Current: v1.0.1\n",
    "\n",
    "7/10/24: 1.0.0: first draft, BN\n",
    "09/10/24: 1.0.1: proofread, MK\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde82c2b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
