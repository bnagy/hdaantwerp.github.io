{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Turn off logging for NUTS sampler for PyMCMC\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"pymc\")\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![laserbayes](./bayes.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Models\n",
    "\n",
    "It is now time to explore the \"other side\" of the most significant ideological debate in applied statistics—Bayesian vs Frequentist statistics. Until now you did not have the terminology and concepts to really understand what is going on here, but now you do.\n",
    "\n",
    "To start out, let's just say that there is no \"right\" way to do inferential statistics. Whichever approach you like better can be used in professional or academic data analysis, and **you should be able to read research that uses either approach**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `bambi`\n",
    "\n",
    "We will install some new software. The [Bambi](https://bambinos.github.io/bambi/) project uses R-style formula language (this is what you have been learning) on top of some other tools to present a fairly intuitive environment. `statsmodels` can do most of the same models, but for many of them it becomes much, *much* harder to understand, and the documentation is written for 'real' statisticians, which makes it challenging.\n",
    "\n",
    "**We really, really, really, (really) recommend installing `bambi` with some kind of conda environment**. Most importantly this should ensure that you get the best versions of BLAS (a hardware accelerated linear algebra library) for your hardware and operating system. This should look something like:\n",
    "\n",
    " `conda install -c conda-forge bambi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import bambi as bmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"...Yet again???\"\n",
    "\n",
    "Instead of explaining everything, we will dive right in, by returning to the first linear regression that we did—the Arthurian Manuscripts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../datasets/arthur/manuscripts.csv\", index_col=0)\n",
    "df.columns = df.columns.str.replace(\"-\", \"_\")\n",
    "\n",
    "page_cols = [\"leaf_height\", \"leaf_width\", \"text_height\", \"text_width\"]\n",
    "mss = df[\n",
    "    page_cols\n",
    "    + [\n",
    "        \"script\",\n",
    "        \"material\",\n",
    "        \"physical_type\",\n",
    "    ]\n",
    "].dropna()\n",
    "mss = mss[mss.script.isin([\"textualis\", \"cursive\"])]\n",
    "\n",
    "# Divide by 10 for mm -> cm\n",
    "mss[page_cols] = mss[page_cols].apply(lambda x: x / 10)\n",
    "\n",
    "mss[\"text_area\"] = mss.text_height * mss.text_width\n",
    "mss[\"leaf_area\"] = mss.leaf_height * mss.leaf_width\n",
    "\n",
    "bad = (\n",
    "    (mss.leaf_height < mss.text_height)\n",
    "    | (mss.leaf_width < mss.text_width)\n",
    "    | ((mss.leaf_area) < 0.001)\n",
    "    | ((mss.text_area) < 0.001)\n",
    ")\n",
    "mss = mss[~bad]\n",
    "mss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Bayesian model **looks** pretty similar in the way we define it. We will model `text_area` using `leaf_area` as a predictor, as you did in the homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bmb.Model(\"text_area ~ leaf_area\", mss)\n",
    "idata = model.fit(target_accept=0.85, random_seed=rng, progressbar=False)\n",
    "az.summary(idata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the output from the homework (just the coefficients section):\n",
    "<pre>\n",
    "==============================================================================\n",
    "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "Intercept     50.2288     10.563      4.755      0.000      29.415      71.043\n",
    "leaf_area      0.4853      0.015     31.967      0.000       0.455       0.515\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the estimates are very similar, but the first thing you will notice is that *there are no more p-values*. This is the most obvious difference with the Bayesian mindset.\n",
    "\n",
    "#### For Bayesian inference, the uncertainty is part of the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other big difference is the appearance of a new model parameter: `sigma`. That's because a Bayesian \"Linear\" Model **isn't really a line**. When we modelled this relationship before, we were trying to determine the *best* fitting line of the form:\n",
    "\n",
    "$\\Huge y=mx+c$\n",
    "\n",
    "In a Bayesian model, we are inferring distributions. We model the dependent variable, $y$ as $Y$ (a *distribution* of predictions) with the form\n",
    "\n",
    "$\\Huge Y \\sim \\mathcal{N}(\\mu, \\sigma^2)$\n",
    "\n",
    "where $\\Large \\mu=\\beta_0 + \\beta_1x_i$\n",
    "\n",
    "Which means: \"the predictions (Y) are normally distributed around a mean that depends on the observations (X) multiplied by some parameter ($\\beta$), with some kind of variance from that expected mean ($\\sigma^2$)\". This variance from the mean is very similar to the **residuals** in our earlier models (remember that we said that the residuals were supposed to be normally distributed for all of our modelling assumptions to hold?).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we 'fit' the model?\n",
    "\n",
    "For a Bayesian model, we have two key concepts. **Priors** are things we believe before we see any observations, and **Posteriors** are the result of adjustments we make based on things we see. Let's look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_priors()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling the Prior\n",
    "\n",
    "Before the model starts fitting, we see that `bambi` has chosen sensible priors for us, they are \"weakly informative\" but the ranges are not totally insane when we look at the final model parameters. We can actually sample from these priors and use the samples to build some possible regression lines! (Don't worry about the code). We will sample the prior and plot some lines using the slopes (`leaf_area` - this is our $\\beta$) and `Intercept` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 points inside the range of our data\n",
    "X = np.linspace(0, mss.leaf_area.max(), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample\n",
    "\n",
    "prior_idata = model.prior_predictive()\n",
    "# The samples are in 4 sublists (one for each chain), mush them into one long column\n",
    "intercepts = np.array(prior_idata.prior[\"Intercept\"]).reshape(-1, 1)\n",
    "slopes = np.array(prior_idata.prior[\"leaf_area\"]).reshape(-1, 1)\n",
    "sigmas = np.array(prior_idata.prior[\"sigma\"]).reshape(-1, 1)\n",
    "\n",
    "# adding and multiplying *as vectors*\n",
    "lines = intercepts + X * slopes\n",
    "lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "\n",
    "sns.set_theme(rc={\"figure.figsize\": (6, 5), \"figure.dpi\": 300}, style=\"white\")\n",
    "lines_samp = rng.choice(lines, 50, replace=False)\n",
    "for y in lines_samp:\n",
    "    sns.lineplot(x=X, y=y, color=\"mediumorchid\", linewidth=0.8)\n",
    "\n",
    "plt.show()\n",
    "sns.reset_defaults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we see that our priors allow positive and negative slopes (but not **too** extreme) and a wide range of `Intercept` values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference!\n",
    "\n",
    "A mathematical description of Bayesian statistics is outside the scope of this course, but the key thing to understand, is that **there is no minimum sample size for Bayesian inference**. Every data point will be used to *update* the prior distribution. That updated distribution is called the posterior. As we make more observations, we *update our prior* each time, and the estimations get tighter and tighter.\n",
    "\n",
    "Now it's time to look at how that happens. This plot shows the fitting process. We used four \"chains\" to do \"sampling\" and we see that the model \"converged\" on the posterior estimates on the left. Notice how the chains (4 colors) all agree, more or less, on the mean estimate, and that the estimate distributions are *much tighter* than the priors. This is because every data point lets the model guess a distribution that is more consistent, while still being aware of **how much uncertainty** there is (the spread of the posterior distributions). Also notice that the posterior estimates are **not** theoretical normals, they are much wigglier! This is because they don't come from a PDF based on estimated parameters, they come right from the data!\n",
    "\n",
    "For example, our `Intercept` prior had a credible interval between -1400 and 2100, but after considering all of the observations, the credible spread is from about 30 to 70."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(idata, compact=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some modeled regression lines again, this time sampling from the **posterior** distributions (after considering the observations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercepts = np.array(idata.posterior[\"Intercept\"]).reshape(-1, 1)\n",
    "slopes = np.array(idata.posterior[\"leaf_area\"]).reshape(-1, 1)\n",
    "sigmas = np.array(idata.posterior[\"sigma\"]).reshape(-1, 1)\n",
    "lines = intercepts + X * slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(rc={\"figure.figsize\": (6, 5), \"figure.dpi\": 300}, style=\"white\")\n",
    "lines_samp = rng.choice(lines, 50, replace=False)\n",
    "for y in lines_samp:\n",
    "    sns.lineplot(x=X, y=y, color=\"mediumorchid\", linewidth=0.6, alpha=0.5)\n",
    "plt.show()\n",
    "sns.reset_defaults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and let's add back our observations!\n",
    "\n",
    "Notice how the sampled regression lines are still all different, but they are much more consistent. We don't know which one of these lines is the \"true mean\" of the distribution, but we can be pretty sure that it lies *somewhere* in the purple area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(rc={\"figure.figsize\": (6, 5), \"figure.dpi\": 300}, style=\"white\")\n",
    "for y in lines_samp:\n",
    "    sns.lineplot(x=X, y=y, color=\"mediumorchid\", linewidth=0.6, alpha=0.3)\n",
    "sns.scatterplot(\n",
    "    x=mss.leaf_area, y=mss.text_area, color=\"steelblue\", marker=r\"$\\circ$\", zorder=2\n",
    ")\n",
    "plt.show()\n",
    "sns.reset_defaults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how the fit changes as we gradually add points. To make the gif, at each step we fitted a new model on partial data, and then took 50 samples from the posterior predictive distribution of means ('bayesian lines'). You can see that the general nature of the relationship is totally unclear at first, but the model gradually tightens its estimates...\n",
    "\n",
    "![scatter](./scatter.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But remember:\n",
    "\n",
    "#### What we are really modelling here is NOT A LINE, it is a DISTRIBUTION OF POSTERIOR PREDICTIONS. The line is just the *mean* value for the normal distribution that drives the predictions:\n",
    "\n",
    "$\\Large \\hat{Y} \\sim \\mathcal{N}(\\mu, \\sigma^2)$\n",
    "\n",
    "where\n",
    "\n",
    "$\\Large \\mu =\\beta_0 + x\\beta_1$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling the Posterior\n",
    "\n",
    "To see this, we will finish by sampling a lot of points from the \"posterior predictive distribution\". You won't be asked to replicate this code, just try to think about what we are doing:\n",
    "- For our sample x-axis values...\n",
    "- multiply by a random slope from the posterior distribution of slopes\n",
    "- add a random intercept from the posterior distribution of intercepts - this is the predicted *mean*\n",
    "- **PREDICT** a Y value for that x-value using a normal distribution, with a variance from the posterior distribution of sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll predict for the entire posterior, and subsample below\n",
    "\n",
    "# reshape X to be a row (maths reasons) so we can 'broadcast' by the slopes column. This gives us a\n",
    "# 2d array with 100 x-axis values, each multiplied by the corresponding slope. This is 'mx' in line\n",
    "# terms.\n",
    "points_y = X.reshape(1, -1) * slopes\n",
    "# Add the intercept column across every row, this is the '+ c'\n",
    "points_y += intercepts\n",
    "# Now map every individual point to one sample from a normal distribution where mu is the 'line\n",
    "# value' (mean estimate). This adds the variation around the mean.\n",
    "points_y = rng.normal(points_y, sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 samples per X value.\n",
    "points_samp = rng.choice(points_y, 20, replace=False).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(rc={\"figure.figsize\": (6, 5), \"figure.dpi\": 300}, style=\"white\")\n",
    "\n",
    "# draw posterior mean lines\n",
    "for y in lines_samp:\n",
    "    sns.lineplot(x=X, y=y, color=\"mediumorchid\", linewidth=0.6, alpha=0.3, zorder=2)\n",
    "\n",
    "# draw posterior (point) predictions\n",
    "ax = sns.scatterplot(\n",
    "    x=np.tile(X, 20),\n",
    "    y=points_samp,\n",
    "    color=\"orange\",\n",
    "    marker=r\"$\\circ$\",\n",
    "    alpha=0.2,\n",
    "    zorder=1,\n",
    ")\n",
    "\n",
    "# draw observations\n",
    "sns.scatterplot(\n",
    "    x=mss.leaf_area, y=mss.text_area, color=\"steelblue\", marker=r\"$\\circ$\", zorder=3\n",
    ")\n",
    "\n",
    "# add legend\n",
    "handles = [\n",
    "    Patch(label=\"Observations\", facecolor=\"steelblue\"),\n",
    "    Patch(label=\"Posterior Predictions\", facecolor=\"orange\"),\n",
    "    Patch(label=\"Posterior Means\", facecolor=\"mediumorchid\"),\n",
    "]\n",
    "ax.axhline(0, linestyle=\"--\", color=\"black\", linewidth=0.7)\n",
    "legend = ax.legend(handles=handles, loc=4, fontsize=8, frameon=True, framealpha=0.8)\n",
    "plt.show()\n",
    "sns.reset_defaults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That was a lot.\n",
    "\n",
    "If you are anything like us, this will have been a lot to take in at first. For now, just remember two things:\n",
    "1. In Bayesian modelling, the **uncertainty is a visible part of the model**\n",
    "2. **The minimum sample size for Bayesian analysis is ONE.** (you will just get very uncertain estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./halfbaked.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Version History\n",
    "\n",
    "Current: v1.0.0\n",
    "\n",
    "14/10/24: 1.0.0: first draft, BN\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bambi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
