{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import bambi as bmb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Turn off logging for NUTS sampler for PyMCMC\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"pymc\")\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Gries, chapter 5.5.4 (pp. 333-336)*\n",
    "\n",
    "Let us illustrate the use of binary logistic **mixed effects** regression with an exercise on teenagers' verb spelling errors (dataset 'spelling')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spelling = pd.read_csv(\n",
    "    \"../../datasets/spelling/spelling.tsv\", sep=\"\\t\", encoding=\"Windows-1252\"\n",
    ")\n",
    "spelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question: Why does the assumption of data independence NOT hold for this dataset? \n",
    "> Where are there repeated measurements in the data? Take a look at both 'subject_ID' and 'lemma'. Using the `is_unique` property of the columns you should easily see that they are not unique. Because there are unbalanced numbers of test-takers, the results would be distorted (it would be like an individual voting many times in an election)\n",
    "\n",
    "In this case, we need to deal with the repeated measurements. The approach is called many different things:\n",
    "- 'controlling for' the repeated individual results\n",
    "- 'stratifying' the data\n",
    "- 'hierarchical' models\n",
    "- 'mixed effects' models\n",
    "- 'random effects' models\n",
    "- ...etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary logistic mixed effects regression\n",
    "\n",
    "Here we need a binary logistic **mixed effects model**. You will notice that the syntax looks almost identical to what you are saw for regular binary logistic models from `statsmodels`. That makes sense, because what we will be fitting now, is just a special ('mixed effects') variant of that same model, that is adapted in a way so that it can deal with repeated measurements in the data.\n",
    "\n",
    "> For Python specifically, as of October 12, 2024, the libraries for modelling and interpreting this kind of problem are easier to understand and better-featured on the Bayesian side. The `statsmodels` version of this type of model also uses Bayesian estimation for the parameters, but presents the results according to the `statsmodels` outlook. The `bambi` version is also Bayesian, but has (in our opinion) a cleaner API and better diagnostics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian vs Frequentist Models\n",
    "\n",
    "There is no (statistically) **better or worse** approach, in a vacuum, to modelling this kind of data. You are about to learn the Bayesian style, but the difference is only in the way the estimates are calculated but (much more importantly) the way they are interpreted. As we said before, both styles work, and both have many fanatical devotees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we want to fit a logistic regression, with a binary outcome. The `bambi` library uses the R style formula language, so that should be familiar. You will see a few things that are new.\n",
    "- The specification `(1|lemma)` and `(1|subject_ID)` creates two *random intercept groups*. This allows the *intercept* to vary between individual students (some students are better than others at spelling) and individual lemmas (some words are harder to spell)\n",
    "- The family `bernoulli` refers to (yet another) distribution, and is the correct choice for logistic models with *binary* outcomes\n",
    "\n",
    "The interpretation of this part of the output falls outside the scope of this course. What does fall within the scope of the course, is: **knowing that you should always test your data for data independence, and subsequently knowing when to switch to a mixed effects rather than a regular regression model or not.**\n",
    "\n",
    "First we specify the model (by formula), and then build it (but don't fit it yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hierarchical = bmb.Model(\n",
    "    \"error ~ 0 + gender + education + grade + (1|lemma) + (1|subject_ID) \",\n",
    "    spelling,\n",
    "    family=\"bernoulli\",\n",
    ")\n",
    "model_hierarchical.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we see that `bambi` has automatically chosen sensible priors for us. Recall that these are all categorical predictors, so what is being estimated is the mean effect of each predictor level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hierarchical.plot_priors(var_names=[\"gender\", \"education\", \"grade\"])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata_hierarchical = model_hierarchical.fit(\n",
    "    target_accept=0.9,\n",
    "    random_seed=rng,\n",
    "    progressbar=False,\n",
    ")\n",
    "az.summary(idata_hierarchical, var_names=[\"gender\", \"education\", \"grade\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting, our estimates have narrowed a lot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(\n",
    "    idata_hierarchical, compact=False, var_names=[\"gender\", \"education\", \"grade\"]\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretation\n",
    "\n",
    "## Predictions and Effects\n",
    "\n",
    "`bambi` provides easy to use tools to plot predictions and effects. When we plot *predictions* it will mariginalize over the variables we give it and (for this kind of model) automatically convert the parameters into probabilities. First let's look at gender vs education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmb.interpret.plot_predictions(\n",
    "    model_hierarchical,\n",
    "    idata_hierarchical,\n",
    "    [\"gender\", \"education\"],\n",
    "    fig_kwargs={\"sharey\": True},\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> EXERCISE: How should we interpret this plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> EXERCISE: Can you add 'grade' to this plot? What happens? What about swapping the order, maybe grade first?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effects\n",
    "\n",
    "We can also plot treatment effects using the `plot_comparisons()` method. Note carefully that this shows us a *change* not an absolute probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, _ = bmb.interpret.plot_comparisons(\n",
    "    model=model_hierarchical,\n",
    "    idata=idata_hierarchical,\n",
    "    contrast=\"gender\",\n",
    "    conditional=[\"education\", \"grade\"],\n",
    ")\n",
    "fig.get_axes()[0].axhline(0.07, linestyle=\"--\", linewidth=0.6, color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should we interpret this? Well, it says, for example, that in vocational education, in Grade 2, applying the gender treatment (from baseline female to male) increases the probability of spelling errors by about 7% *on average*, but there is a lot of uncertainty in the estimate. Let's check that with a raw predictions table...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = bmb.interpret.predictions(\n",
    "    model_hierarchical,\n",
    "    idata_hierarchical,\n",
    "    [\"gender\", \"education\", \"grade\"],\n",
    ")\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = preds[(preds.education == \"vocational\") & (preds.grade == \"Gr2\")]\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.iloc[1, 5:] - subset.iloc[0, 5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> REMEMBER: In Bayesian analysis, the estimate is the *entire distribution*. When you report predictions and/or effects you should always report some kind of uncertainty. The ideal way for Bayesian models is to plot the full posterior. If you can't, you can *summarize* the estimate by using eg the bounds of the 94% HDI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Version History\n",
    "\n",
    "Current: v1.0.0\n",
    "\n",
    "17/11/24: 1.0.0: first draft, BN\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bambi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
