{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import bambi as bmb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Turn off logging for NUTS sampler for PyMCMC\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"pymc\")\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating and Selecting Models—Bayesian Edition\n",
    "\n",
    "Last session, we looked at some tools for model selection with frequentist models. In particular, we looked at AIC and BIC as tools to compare models.\n",
    "\n",
    "For Bayesian models, the 'confidence' of the model is easily seen by the spread of the estimates, however this is different to the model's performance. To assess model performance we recommend the ELPD ('expected log predictive density'). This can be calculated *either* with LOO (leave-one-out) for with WAIC (widely applicable information criterion); we recommend LOO.\n",
    "\n",
    "To explain the intuition behind ELPD, consider the plot from the first sprint:\n",
    "\n",
    "![plot](./linear_preds.png)\n",
    "\n",
    "For observations (blue) that are in areas with a lot of predictions (orange), the *predictive density* is high. For observations by themselves (few predictions) the predictive density is low. By leaving out each observation in turn (LOO) and considering the density in that area, we get a feel for how well the model predicts in general.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1—Using the wrong model\n",
    "\n",
    "First we'll look at what happens if we use a model without mixed effects for the data from last sprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spelling = pd.read_csv(\n",
    "    \"../../datasets/spelling/spelling.tsv\", sep=\"\\t\", encoding=\"Windows-1252\"\n",
    ")\n",
    "spelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: To perform the ELPD comparisons, we need to tell the fitting method to preserve \"log-likelihood outcomes\", using the slightly weird incantation:\n",
    "\n",
    "`idata_kwargs={\"log_likelihood\": True}`\n",
    "\n",
    "(this is not done by default because it's a little bit slower. An unfortunate choice in our view...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hierarchical = bmb.Model(\n",
    "    \"error ~ 0 + gender + education + grade + (1|lemma) + (1|subject_ID) \",\n",
    "    spelling,\n",
    "    family=\"bernoulli\",\n",
    ")\n",
    "idata_hierarchical = model_hierarchical.fit(\n",
    "    target_accept=0.9,\n",
    "    random_seed=rng,\n",
    "    progressbar=False,\n",
    "    idata_kwargs={\"log_likelihood\": True},\n",
    ")\n",
    "az.summary(idata_hierarchical, var_names=[\"gender\", \"education\", \"grade\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_non_hierarchical = bmb.Model(\n",
    "    \"error ~ 0 + gender + education + grade \",\n",
    "    spelling,\n",
    "    family=\"bernoulli\",\n",
    ")\n",
    "idata_non_hierarchical = model_non_hierarchical.fit(\n",
    "    progressbar=False,\n",
    "    idata_kwargs={\"log_likelihood\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we use a \"forest plot\" to compare the \"high density interval\" for each predictor. This plot tells us how confident the models are in their estimates. \n",
    "\n",
    "Here we have added a custom \"transform\" to graph the results as Odds (which are easier to understand that logistic model parameters), but it is only for clarity. The main thing to notice is that the most likely parameters for the two models are not _so_ different, but the spread changes a lot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Create forestplot with ArviZ, only for the mean.\n",
    "az.plot_forest(\n",
    "    [\n",
    "        idata_hierarchical,\n",
    "        idata_non_hierarchical,\n",
    "    ],\n",
    "    var_names=[\"gender\", \"education\", \"grade\"],\n",
    "    combined=True,\n",
    "    colors=[\"orange\", \"mediumorchid\"],\n",
    "    transform=lambda x: np.exp(x),\n",
    "    linewidth=2.6,\n",
    "    markersize=8,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "# Create legend\n",
    "handles = [\n",
    "    Patch(label=\"Hierarchical\", facecolor=\"orange\"),\n",
    "    Patch(label=\"Non-Hierarchical\", facecolor=\"mediumorchid\"),\n",
    "]\n",
    "\n",
    "ax.set(\n",
    "    title=\"94% HDI - Odds Ratio\",\n",
    ")\n",
    "ax.axvline(1, linestyle=\"--\", color=\"black\", linewidth=0.7)\n",
    "legend = ax.legend(handles=handles, loc=4, fontsize=14, frameon=True, framealpha=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> EXERCISE: How do you interpret the Odds Ratios?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidently Wrong\n",
    "\n",
    "Note how the Non-Hierarchical model has tighter estimates! This is because from its point of view there are **many more observations** because the students are not grouped. In other words the students for whom there are many records (they were tested on many words) are treated as if they are all different people. This not only makes the model incorrect (because of bias) but also makes it more confident (because it thinks it has more observations).\n",
    "\n",
    "> IMPORTANT Just because a model has tight estimates does not mean that it is good or correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELPD Comparison Plot\n",
    "There is a built in plot to calculate the ELPD, and compare the predictive power of the models using the LOO method as described. This will automatically rank the models. Higher ELPD is better (and the plot arranges them so the models are ranked from top to bottom). The theory is very complicated, but it is similar 'in spirit' to the AIC/BIC measures. In particular, the ELPD also considers the effective number of parameters to automatically penalise models with more parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = az.compare(\n",
    "    {\n",
    "        \"heirarchical\": idata_hierarchical,\n",
    "        \"non-heirarchical\": idata_non_hierarchical,\n",
    "    }\n",
    ")\n",
    "az.plot_compare(comp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2—Model Comparison\n",
    "\n",
    "Once again, opinions differ regarding 'exploratory' analysis, but for the sake of argument, here we repeat the model comparison from last session, but in Bayesian style. Because the models can take a while to fit, we compare just the top 10 models from the AIC/BIC comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../datasets/gries/05-2_reactiontimes.csv\", sep=\"\\t\")\n",
    "df[\"LENGTH\"] = df.CASE.str.len()\n",
    "df[\"IMAGEABILITY\"] = df.IMAGEABILITY.astype(\n",
    "    pd.api.types.CategoricalDtype(categories=[\"lo\", \"hi\"], ordered=True)\n",
    ")\n",
    "df[\"FAMILIARITY\"] = df.FAMILIARITY.astype(\n",
    "    pd.api.types.CategoricalDtype(categories=[\"lo\", \"med\", \"hi\"], ordered=True)\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"FAMILIARITY + MEANINGFULNESS + LENGTH\",\n",
    "    \"MEANINGFULNESS + LENGTH\",\n",
    "    \"MEANINGFULNESS*LENGTH\",\n",
    "    \"FREQUENCY + FAMILIARITY + MEANINGFULNESS + LENGTH\",\n",
    "    \"FAMILIARITY + IMAGEABILITY + MEANINGFULNESS + LENGTH\",\n",
    "    \"FREQUENCY + MEANINGFULNESS + LENGTH\",\n",
    "    \"IMAGEABILITY + MEANINGFULNESS + LENGTH\",\n",
    "    \"FREQUENCY + FAMILIARITY + IMAGEABILITY + LENGTH\",\n",
    "    \"FREQUENCY + IMAGEABILITY + MEANINGFULNESS + LENGTH\",\n",
    "    \"FAMILIARITY + IMAGEABILITY + LENGTH\",\n",
    "    \"FREQUENCY + FAMILIARITY + LENGTH\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run each model, store each inference data object in a dict\n",
    "\n",
    "results = {}\n",
    "for m in models:\n",
    "    print(f\"Fitting: {m}\")\n",
    "    model = bmb.Model(\n",
    "        f\"RT ~ {m}\",\n",
    "        df.dropna(),\n",
    "    )\n",
    "    idata = model.fit(\n",
    "        progressbar=False,\n",
    "        idata_kwargs={\"log_likelihood\": True},\n",
    "    )\n",
    "    results[m] = idata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Plot\n",
    "\n",
    "The plots have two components:\n",
    "- ELPD (with credible interval) is the overal predictive power of the model\n",
    "- ELPD difference (in grey) is the **difference** from the reference (best) model.\n",
    "\n",
    "The ELPD difference is probably the most important. Note that the `FAMILIARITY + MEANINGFULNESS + LENGTH` model is probably worse than the reference model, but that the difference line crosses over, so it is not certainly worse. The second-best model by ELPD (`FREQUENCY + MEANINGFULNESS + LENGTH`) is almost certainly worse (but not by much). These results are qualitatively similar to the conclusions we drew last session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = az.compare(results)\n",
    "az.plot_compare(comp, figsize=(12, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compare the estimates from the models for `FAMILIARITY + MEANINGFULNESS + LENGTH`. Here are the results from the statsmodels linear regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "===============================================================================================\n",
    "                                  coef    std err          t      P>|t|      [0.025      0.975]\n",
    "-----------------------------------------------------------------------------------------------\n",
    "Intercept                     613.6193     69.711      8.802      0.000     473.034     754.204\n",
    "C(FAMILIARITY, Diff)[D.lo]    -24.3092     17.225     -1.411      0.165     -59.046      10.428\n",
    "C(FAMILIARITY, Diff)[D.med]   -10.2538     14.383     -0.713      0.480     -39.260      18.753\n",
    "MEANINGFULNESS                 -0.1355      0.157     -0.863      0.393      -0.452       0.181\n",
    "LENGTH                         11.1608      3.006      3.713      0.001       5.099      17.222\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and here are the results from `bambi`. As you can see, the estimates and the credible intervals (or confidence intervals on the statsmodels side) are very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(results[\"FAMILIARITY + MEANINGFULNESS + LENGTH\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Summary...\n",
    "\n",
    "When comparing model results you should consider BOTH the uncertainty of the models *and* their predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Version History\n",
    "\n",
    "Current: v1.0.0\n",
    "\n",
    "17/11/24: 1.0.0: first draft, BN\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bambi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
