{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized linear models: Poisson regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Gries, chapter 5.4.3 (pp. 324-327)*\n",
    "\n",
    "Let us revisit the exercise on teenagers' submission/donation of chat conversations to a sociolinguistic research project. Instead of running a binary logistic regression (for response: 'small' vs 'large' submission), we will now try to predict the actual number of words donated. **For modeling (unbounded) counts or frequencies, we use count regression**. **Recall that a 'regular' linear model is not adequate to model counts** as its predictions can range from $-\\infty$ to $+\\infty$, and a negative count does not make any sense (nor do floating-point numbers). Count distributions always have a 'hard floor' at zero, as no negative counts occur. Additionally, they are guaranteed to be integers. \n",
    "\n",
    "The prototypical count distribution is a *Poisson distribution*, named after the French mathematician Poisson - hence the name 'Poisson regression' for the modeling of counts and frequencies. However, Poisson models do not deal well with overdispersed counts, which tend to happen a lot. The correct choice of model is a matter of some debate. For this course, *in general* we recommend that you model basic count data with Bambi using `family='negativebinomial'`, but you can always test with `family=poisson` and see if the model improves!\n",
    "\n",
    "> MATHS: You can read more about modelling with negative binomials [in the Bambi examples](https://bambinos.github.io/bambi/notebooks/negative_binomial.html) but the short version is that negative binomial distributions have an extra parameter when compared to Poisson, which allows them to fit with longer tails. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import bambi as bmb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Turn off logging for NUTS sampler for PyMCMC\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"pymc\")\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(\n",
    "    {\"mathtext.default\": \"regular\", \"figure.dpi\": 300, \"figure.figsize\": (8, 8)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and inspecting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Outliers\n",
    "\n",
    "**We stress that it is heavily debated whether or not such outliers should be deleted prior to Poisson regression.** While some argue that you should delete outliers as they may have a too strong influence on the modeling, others argue that outliers in Poisson distributions are really not that bad (compared to other distributions), and that all 'valid' datapoints should just be kept in. \n",
    "\n",
    "Here, to illustrate once more the procedure of deleting outliers, we show you how to delete them, but in the end we run all the analysis on the full data. A good exercise would be to compare the results on the cut data.\n",
    "\n",
    "A widely applied cutoff to delete outliers, is to delete items that are 1.5 times or more the interquartile range (IQR) above the third quartile, and 1.5 times or more the IQR below the first quartile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = pd.read_csv(\"../../datasets/chat/chat.tsv\", sep=\"\\t\")\n",
    "\n",
    "iqr = chat.nr_tokens.quantile(0.75) - chat.nr_tokens.quantile(0.25)\n",
    "cut = chat.nr_tokens.quantile(0.75) + iqr * 1.5\n",
    "\n",
    "chat_cut = chat[chat.nr_tokens < cut]\n",
    "print(f\"{chat.shape[0]- chat_cut.shape[0]} observations removed.\")\n",
    "chat_cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model\n",
    "\n",
    "The only change we need is to specify the model family - here it is `family=negativebinomial`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bmb.Model(\n",
    "    \"nr_tokens ~ gender\",\n",
    "    chat,\n",
    "    family=\"negativebinomial\",\n",
    ")\n",
    "idata = model.fit(\n",
    "    target_accept=0.9,\n",
    "    random_seed=rng,\n",
    "    idata_kwargs={\"log_likelihood\": True},\n",
    "    progressbar=False,\n",
    ")\n",
    "az.summary(idata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check chains and posterior estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(idata, compact=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check predicted means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmb.interpret.plot_predictions(\n",
    "    model,\n",
    "    idata,\n",
    "    [\"gender\"],\n",
    "    fig_kwargs={\"sharey\": True},\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE the predicted means come from the mean estimates, which are on a log scale. The intercept (women are the base class) was 7.770, To convert log values to numbers we use `np.exp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This matches the mean prediction from the plot\n",
    "\n",
    "np.exp(7.770)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Posterior Predictive Curves\n",
    "\n",
    "It would be good to know that our model predictions look something like the data. Here we use some complicated code, modified from the [Bambi Examples](https://bambinos.github.io/bambi/notebooks/count_roaches.html). Don't worry about the code too much. The key thing to look at is whether the `observed` curve (our real observations) falls mostly within the posterior predictive estimates (could it plausibly be predicted by the model).\n",
    "\n",
    "The predictions are occasionally very large, so here we cut some *predictions* above the 3rd Quartile to make the result easier to see. Overall, the predictions don't seem terrible. We are under-predicting for low token counts, but not drastically. Note that we did NOT remove outliers from the observations, the \"cutting\" here is only so we can see the curve fitting at the low end where most of the observations lie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_posterior_ppc(model, idata, cut=False):\n",
    "    # plot posterior predictive check\n",
    "    id = model.predict(idata, kind=\"response\", inplace=False)\n",
    "    var_name = \"nr_tokens\"\n",
    "    # there is probably a better way\n",
    "    ppd = id.posterior_predictive[\"nr_tokens\"]\n",
    "    obs = id.observed_data[\"nr_tokens\"]\n",
    "    if cut:\n",
    "        id.posterior_predictive[var_name] = ppd.where(ppd < cut, drop=True)\n",
    "        id.observed_data[var_name] = obs.where(obs < cut)\n",
    "    else:\n",
    "        id.posterior_predictive[var_name] = ppd\n",
    "        id.observed_data[var_name] = obs\n",
    "\n",
    "    return az.plot_ppc(id, var_names=[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can increase the quantile value to see the fit on more of the observations\n",
    "\n",
    "plot_posterior_ppc(model, idata, cut=chat.nr_tokens.quantile(0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a model with education terms\n",
    "\n",
    "We can add categorical predictors just as easily..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_education = bmb.Model(\n",
    "    \"nr_tokens ~ gender + education\",\n",
    "    chat,\n",
    "    family=\"negativebinomial\",\n",
    ")\n",
    "idata_education = model_education.fit(\n",
    "    target_accept=0.9,\n",
    "    random_seed=rng,\n",
    "    idata_kwargs={\"log_likelihood\": True},\n",
    "    progressbar=False,\n",
    ")\n",
    "az.summary(idata_education)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmb.interpret.plot_predictions(\n",
    "    model_education,\n",
    "    idata_education,\n",
    "    [\"gender\", \"education\"],\n",
    "    fig_kwargs={\"sharey\": True},\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for interaction effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_interaction = bmb.Model(\n",
    "    \"nr_tokens ~ gender * education\",\n",
    "    chat,\n",
    "    family=\"negativebinomial\",\n",
    ")\n",
    "idata_interaction = model_interaction.fit(\n",
    "    target_accept=0.9,\n",
    "    random_seed=rng,\n",
    "    idata_kwargs={\"log_likelihood\": True},\n",
    "    progressbar=False,\n",
    ")\n",
    "az.summary(idata_interaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> QUESTION: What do you think? Are the modelled interaction effects significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(idata_interaction, compact=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmb.interpret.plot_predictions(\n",
    "    model_interaction,\n",
    "    idata_interaction,\n",
    "    [\"gender\", \"education\"],\n",
    "    fig_kwargs={\"sharey\": True},\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The failure of the Poisson model\n",
    "\n",
    "> NOTE CAREFULLY that a model can still produce reasonable *mean* estimates, and (correctly) decide that effects are real while still doing a terrible job of modelling. We illustrate that here by using a Poisson model on the (massively overdispersed) data and then looking at the posterior predictive distributions.\n",
    "\n",
    "> In Bayesian statistics, the output is THE WHOLE DISTRIBUTION, not a point estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_poisson = bmb.Model(\n",
    "    \"nr_tokens ~ gender + education\",\n",
    "    chat,\n",
    "    family=\"poisson\",\n",
    ")\n",
    "idata_poisson = model_poisson.fit(\n",
    "    target_accept=0.9,\n",
    "    random_seed=rng,\n",
    "    idata_kwargs={\"log_likelihood\": True},\n",
    "    progressbar=False,\n",
    ")\n",
    "az.summary(idata_poisson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_posterior_ppc(model_poisson, idata_poisson, cut=chat.nr_tokens.quantile(0.75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing models\n",
    "\n",
    "Finally, we look at the ELPD of the three negative binomial models. As we saw, the interaction effects are not useful, and so we see that the `gender * education` model is a little weaker than the `gender + education` model. This conflicts with what we saw with the logistic models, by the way, and it is probably because the very long tail makes it hard for the model to accurately detect the education interaction effect. You could try eliminating `nr_tokens` outliers above 3Q + 1.5*IQR and see what happens..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = az.compare(\n",
    "    {\n",
    "        \"gender\": idata,\n",
    "        \"gender + education\": idata_education,\n",
    "        \"gender * education\": idata_interaction,\n",
    "    }\n",
    ")\n",
    "az.plot_compare(comp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Version History\n",
    "\n",
    "Current: v1.0.0\n",
    "\n",
    "17/11/24: 1.0.0: first draft, BN\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bambi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
