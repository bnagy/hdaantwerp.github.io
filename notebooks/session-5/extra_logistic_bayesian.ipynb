{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized linear models: Binary logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import arviz as az\n",
    "import bambi as bmb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update(\n",
    "    {\"mathtext.default\": \"regular\", \"figure.dpi\": 300, \"figure.figsize\": (8, 8)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this silences some warnings from bambi/pandas\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"pymc\")\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Gries, chapter 5.3 (pp. 293-316)*\n",
    "\n",
    "Let us revisit a previous exercise on teenagers' ('large' vs 'small') submission of chat conversations to a sociolinguistic research project. \n",
    "Instead of performing a chi-squared test, we will now run a binary logistic regression. Important distinctions with some simpler correlation tests are that a **binary logistic regression** model:\n",
    "- includes a direction: response (output) variable versus predictor (input) variables\n",
    "- can include many types of **predictor** variables, not just categorical ones (**the response variable, however, is always binary!**)\n",
    "- allows us to inspect the impact of multiple predictors simultaneously\n",
    "- and to include interactions between them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and inspecting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and inspect 'chat.tsv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = pd.read_csv(\"../../datasets/chat/chat.tsv\", sep=\"\\t\")\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question: Does the assumption of data independence hold? \n",
    "> Are there repeated measurements in the data, i.e. >1 observation for 1 subject? This is important to check, since **data independence is an assumption of regression models without random effects** (as well as of many other statistical tests). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.subject_ID.is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a new binary variable: was the participant's submission to the research project large (>500 tokens of chat data) or small (<=500 tokens)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat[\"sub_large\"] = chat.nr_tokens > 500\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Logit?\n",
    "\n",
    "You will often see people talk about 'logits' or 'logit link functions'. These terms come from a special curve which is often used to describe the probability of **binary outcomes**. In other words, the output *must* be $A$ or $B$, but the relative probabilities of the two outcomes is *continuous*. The function is:\n",
    "\n",
    "$\\Large\\frac{1}{ 1 + e^{-x}}$\n",
    "\n",
    "The curve looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 1000)\n",
    "y = 1 / (1 + np.exp(-x))\n",
    "ax = sns.lineplot(x=x, y=y)\n",
    "ax.axhline(0.5, linewidth=0.5, color=\"black\")\n",
    "ax.axvline(0, linewidth=0.5, color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason logits are important is that it is common (for maths reasons) for models to output *exponential parameters* but mostly as humans we would prefer to read these are *pure probabilities* or *odds ratios*, as we have done earlier in the course. The logit lets us convert between those domains. This is also, by the way, why it is called 'logistic regression'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with `ols()`, the model fits a **regression line** that it will use to make predictions. Only two numbers are used to mathematically define a line: an **intercept** ('Intercept' in the output) and a **slope** for the line (the predictor coefficient in the output, here 'gendermale'). For a binary predictor, the regression line does not run through all the datapoints, but rather connects the estimated output for the predictor levels: so here, the regression line runs through the output (**chance** of a large submission) for female and male participants (which are the two levels of the predictor 'gender').\n",
    "\n",
    "> MATHS: Recall that a straight line in log space is a curvy exponential line in 'normal' numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary logistic regression with one binary predictor\n",
    "\n",
    "Let's now move on to a **binary logistic regression model**. You will notice that the syntax looks a lot like what you are already used to for linear modeling with `ols()`. That makes sense, because what we will be fitting now, is just a special ('generalized') variant of linear regression, that is adapted in a way so that it makes sense for a binary (rather than a numeric) response variable.\n",
    "\n",
    "HOWEVER, like the linear model we just did, this will use Bayesian parameter estimation.\n",
    "\n",
    "First, we only include gender as a predictor.\n",
    "\n",
    "### Model fitting\n",
    "\n",
    "We fit a bambi model using `family=\"bernoulli\"` and the same R-style formula language we have been learning. (As with statsmodels, this formula language support is provided by the `patsy` package).\n",
    "\n",
    "Then inspect the model with `summary()`. Again, we have the Bayesian style model diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bmb.Model(\n",
    "    \"sub_large ~ gender\",\n",
    "    chat,\n",
    "    family=\"bernoulli\",\n",
    ")\n",
    "idata = model.fit(\n",
    "    target_accept=0.9,\n",
    "    random_seed=rng,\n",
    "    idata_kwargs={\"log_likelihood\": True},\n",
    "    progressbar=False,\n",
    ")\n",
    "az.summary(idata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Model Diagnostics\n",
    "\n",
    "Apart from the estimates, the main diagnostics here tell us how well the model explored the parameter space. Usually if anything is really bad, the software will warn you, and a full dive into tuning Bayesian estimation is beyond the scope of this course. However, here are the concepts:\n",
    "- The `mcse` is \"Monte-Carlo standard error\" and it tell us how well the chains have sampled the parameter space. Low numbers are better.\n",
    "- the `ess` is the \"effective sample size\", and it tells us how much information the sampling process was able to extract. Higher numbers are better. Some sources recommend an ESS of at least 1000, and less than 100 is almost certainly bad.\n",
    "- the `r_hat` is a measure of how well the 'chains' have converged (how well they agree on the parameter estimates). The ideal value is 1, or very close to 1, anything else is cause for concern.\n",
    "\n",
    "Again, note that Bayesian modelling does not use $p$-values. We consider an effect to be 'significant' if it is almost certainly non-zero. For example, the effect of `gender[male]` has 94% of its High Density Interval (hdi) below zero. If we wanted, we could calculate a summary statistic for the estimate (we have the mean and standard deviation of an estimated normal distribution, we did this in session 2) but the Bayesian philosophy is simply to report the credible interval for the estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, we can already derive from the coefficients table that male participants will make $fewer$ large donations compared to female participants, as the estimate for `gender[male]` is negative. (So this means: the regression line between `gender[female]` and `gender[male]` goes down, moving from the reference group to the non-reference group, as it has a negative slope). In combination with the HDI, we can already conclude that boys make fewer large donations than girls. In some analyses, this interpretation will suffice. However, sometimes, you may like to get actual probabilities for your predictor levels from the model. You can obtain these in multiple ways, but the methods `interpret.predictions()` and `interpret.plot_predictions()` will automatically convert to probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = bmb.interpret.predictions(model, idata, average_by=\"gender\")\n",
    "tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that because this is the simplest possible model, the predictions look at lot like the empirical probabilities in the data. Here we show a crosstab, and note that the `True` value for `sub_large` (which is what is being modelled) looks almost the same as our model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(chat.gender, chat.sub_large, normalize=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> MATHS: Out of interest, we can convert the model parameters to probabilities ourselves using the logit link function (just to make sure it works)\n",
    ">\n",
    "> $\\Large\\frac{1}{ 1 + e^{-x}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intercept mean estimate (base category is female)\n",
    "x = az.summary(idata)[\"mean\"][\"Intercept\"]\n",
    "# use as the parameter for logit link\n",
    "1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method you will need to know, though, is to use the predictions table or plot, as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmb.interpret.plot_predictions(\n",
    "    model,\n",
    "    idata,\n",
    "    [\"gender\"],\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary logistic regression with a categorical predictor\n",
    "\n",
    "Let us now try to improve the model by adding the three-level categorical variable 'education' as predictor. We use the formula API as before and simply add education as an independent predictor with `+`\n",
    "\n",
    "`sub_large ~ gender + education`\n",
    "\n",
    "Here we use an optional argument when fitting `idata_kwargs={\"log_likelihood\": True}` which we need to be able to run model comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ed = bmb.Model(\n",
    "    \"sub_large ~ gender + education\",\n",
    "    chat,\n",
    "    family=\"bernoulli\",\n",
    ")\n",
    "idata_ed = model_ed.fit(\n",
    "    random_seed=rng,\n",
    "    idata_kwargs={\"log_likelihood\": True},\n",
    "    progressbar=False,\n",
    ")\n",
    "az.summary(idata_ed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effect of education is almost certainly not zero, so we conclude that it is associated with the outcome. Note that `general` has been chosen automatically as the reference level (just alphabetically), so the parameters are relative to that.\n",
    "\n",
    "> EXERCISE: What does this mean? Remember what we are modelling, and then complete this sentence: \"Based on the parameter estimates, we expect students in technical education to be....\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmb.interpret.plot_predictions(\n",
    "    model_ed,\n",
    "    idata_ed,\n",
    "    [\"education\", \"gender\"],\n",
    "    fig_kwargs={\"sharey\": True},\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> EXERCISE: Repeat the plot, but level first by gender, then by education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmb.interpret.plot_predictions(\n",
    "    model_ed,\n",
    "    idata_ed,\n",
    "    [\"gender\", \"education\"],\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating and Selecting Models—Bayesian Edition\n",
    "\n",
    "Last session, we looked at some tools for model selection with frequentist models. In particular, we looked at AIC and BIC as tools to compare models.\n",
    "\n",
    "For Bayesian models, the 'confidence' of the model is easily seen by the spread of the estimates, however this is different to the model's performance. To assess model performance we recommend the ELPD ('expected log predictive density'). This can be calculated *either* with LOO (leave-one-out) or with WAIC (widely applicable information criterion); we recommend LOO.\n",
    "\n",
    "To explain the intuition behind ELPD, consider the plot from the first sprint:\n",
    "\n",
    "![plot](./linear_preds.png)\n",
    "\n",
    "For observations (blue) that are in areas with a lot of predictions (orange), the *predictive density* is high. For observations by themselves (few predictions) the predictive density is low. By leaving out each observation in turn (LOO) and considering the density in that area, we get a feel for how well the model predicts in general.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELPD Comparison Plot\n",
    "There is a built in plot to calculate the ELPD, and compare the predictive power of the models using the LOO method as described. This will automatically rank the models. Higher ELPD is better (and the plot arranges them so the models are ranked from top to bottom). The theory is very complicated, but it is similar 'in spirit' to the AIC/BIC measures. In particular, the ELPD also considers the effective number of parameters to automatically penalise models with more parameters.\n",
    "\n",
    "## In Summary:\n",
    "- HIGHER ELPD estimates mean better models\n",
    "- Like AIC/BIC, ELPD is only useful to compare *different models for the same thing* to see which model has more predictive power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = az.compare(\n",
    "    {\n",
    "        \"gender\": idata,\n",
    "        \"gender + education\": idata_ed,\n",
    "    }\n",
    ")\n",
    "az.plot_compare(comp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactions\n",
    "\n",
    "The last thing to explore for this model is the *interaction* between `gender` and `education`. When we added `+ education`, we were telling the model that we believe these predictors were *conditionally independent*. The [wikipedia article](https://en.wikipedia.org/wiki/Conditional_independence) (for once) is quite good at the start, but the intuition is even simpler. We told the model that the effect of gender on submission size was the same for all education levels (that the effect of education was *independent*).\n",
    "\n",
    "Look back at your plots above—you see that the relative 'shapes' of the predictions per column are the same, and the (imaginary) lines connecting the points would all be roughly parallel.\n",
    "\n",
    "We don't actually know if this is the case. Let's find out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction Terms\n",
    "\n",
    "So, let us include the **interaction** between our two predictors (indicated in the formula with `*`): the goal is to investigate whether the effect of one predictor on the response is affected by the other predictor in some way. E.g. the influence of gender on submission size may differ depending on the participant's educational track. Or the impact of education on submission size may be different for boys versus girls.\n",
    "\n",
    "Fit the model and look at the summary. In the output table, you will now not only see estimates for the main effects (gender and education), but for their interaction too. An interaction is added by combining predictors with `*` rather than with `+`(which is just an addition of main effects, no interaction). In this formula language, the syntax\n",
    "\n",
    "> response ~ predictorA * predictorB\n",
    "\n",
    "is shorthand for\n",
    "\n",
    "> response ~ predictorA + predictorB + predictorA:predictorB\n",
    "\n",
    "We note that when reporting models it is considered bad practice to only include the part `predictorA:predictorB`, and thus exclude the actual main effects that create the interaction. So, **when including an interaction, always use the `*` syntax!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_interact_ed = bmb.Model(\n",
    "    \"sub_large ~ gender*education\",\n",
    "    chat,\n",
    "    family=\"bernoulli\",\n",
    ")\n",
    "idata_interact_ed = model_interact_ed.fit(\n",
    "    target_accept=0.9,\n",
    "    random_seed=rng,\n",
    "    idata_kwargs={\"log_likelihood\": True},\n",
    "    progressbar=False,\n",
    ")\n",
    "az.summary(idata_interact_ed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we visualise the estimates for you using a **forest plot**, converting the estimates to Odds to make them easier to understand (recall that an odds ratio of 1 means \"equally likely\")\n",
    "\n",
    "When looking at interactions, the rule is that if an interaction effect is significant, we don't analyse the predictors independently any more.\n",
    "\n",
    "> EXERCISE: Interpret the plot. Do you think that `education` and `gender` are conditionally independent, or does the interaction have an effect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "az.plot_forest(\n",
    "    [\n",
    "        idata_interact_ed,\n",
    "    ],\n",
    "    combined=True,\n",
    "    colors=[\"orange\", \"mediumorchid\"],\n",
    "    transform=lambda x: np.exp(x),\n",
    "    linewidth=2.6,\n",
    "    markersize=8,\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title(\"94% HDI—Odds Ratio\")\n",
    "ax.axvline(1, linestyle=\"--\", linewidth=0.7, color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare our three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = az.compare(\n",
    "    {\n",
    "        \"gender\": idata,\n",
    "        \"gender + education\": idata_ed,\n",
    "        \"gender * education\": idata_interact_ed,\n",
    "    }\n",
    ")\n",
    "az.plot_compare(comp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the ELPD difference, it is not certain that the interaction improves the model, but it seems pretty likely. Let's see now what the predictions look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmb.interpret.plot_predictions(\n",
    "    model_interact_ed,\n",
    "    idata_interact_ed,\n",
    "    [\"education\", \"gender\"],\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> EXERCISE: Interpret the prediction output above. Re-arrange the predictors (by gender then education). Interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmb.interpret.plot_predictions(\n",
    "    model_interact_ed,\n",
    "    idata_interact_ed,\n",
    "    [\"gender\", \"education\"],\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use `plot_comparisons()` to directly plot the *effect* of gender, **conditional on education**. The way to read this plot is \"for each education level, what does changing from male to female do to the **odds** of donating a large submission. You won't be asked to reproduce one of these plots, but you may need to interpret them.\n",
    "\n",
    "> EXERCISE: Based on the plot below, complete the following sentence: \"In general education, men and women are roughly equally likely to make a large submission. However ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = bmb.interpret.plot_comparisons(\n",
    "    model_interact_ed,\n",
    "    idata_interact_ed,\n",
    "    contrast={\"gender\": [\"male\", \"female\"]},\n",
    "    conditional=\"education\",\n",
    "    comparison_type=\"ratio\",\n",
    ")\n",
    "axs[0].set_title(r\"Contrast Male $\\rightarrow$ Female, Odds Ratio\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Version History\n",
    "\n",
    "Current: v1.0.0\n",
    "\n",
    "22/10/24: 1.0.0: first draft, BN\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bambi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
